
##############################################################################   PENDING ##################################
1. case class 
2. Data frames
	https://hortonworks.com/hadoop-tutorial/a-lap-around-apache-spark/
3. ORC
	https://hortonworks.com/hadoop-tutorial/using-hive-with-orc-from-apache-spark/

4. Apache Spark - Scala - Externalizing Parameters (To pass arguments from a file)
https://www.youtube.com/watch?v=ln4dpChqJPI&list=PLf0swTFhTI8pgc4wyhT8lIEJCPT08RI3X&index=35

	# To pass the arguments from a file, NOTE the version for typesafe
	libraryDependencies += "com.typesafe" % "config" % "1.3.0"

5. Practice: http://discuss.itversity.com/t/find-the-following-from-the-given-products-csv-for-those-who-are-preparing-for-cca175/2516	
6. Environment Setup & Practice:
	https://community.hortonworks.com/questions/64278/hdpcd-spark-certification.html

7. Dataset 	
#################################################################### Final preparation  ###########################
1. Setup Hortonwork environment
2. Configure Spark properties
3. Make sure all the instructions given in this link are practiced
https://hortonworks.com/services/training/certification/exam-objectives/#hdpcdspark
4. Use Hive with ORC from Spark SQL
5. Hortonworks has model exams for other certification, refer to that to understand the exam type 

#################################################################### About HDCPD-spark # Some reference ###########################

HDPCD - Spark Certification  
Hortonworks Data Platform (HDP)

If you get disconnected at any time, log back in to examslocal.com and attempt to re-launch the exam. The exam proctor will take over from there and assist in getting you back up and running.

https://hortonworks.com/services/training/certification/hdp-certified-spark-developer/
https://hortonworks.com/tutorials/
https://hortonworks.com/services/training/certification/exam-objectives/#hdpcdspark
https://www.youtube.com/watch?v=dyGABwUMgso&list=PLf0swTFhTI8pgc4wyhT8lIEJCPT08RI3X
http://discuss.itversity.com/c/certifications/hdpcd-spark
https://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/
https://spark.apache.org/docs/1.6.1/configuration.html

Do NOT buy, just for reference : http://www.validdumps.com/HDPCD-Spark
Do NOT buy, just for reference : http://www.hadoopexam.com/HORTONWORKS_CERT/What_is_hortonworks_hdpcd_spark_Certification.html


#################################################################### Setup the Environment ###########################
http://discuss.itversity.com/t/step-01-setup-hadoop-and-spark-environment-as-well-as-ide/2407
https://www.youtube.com/watch?v=TP7YY7UCwe8

#################################################################### About HDCPD-spark Exam ###########################

https://community.hortonworks.com/questions/49270/queries-about-the-hdpcdspark-exam.html
https://community.hortonworks.com/questions/45535/hdpcd-spark-environment.html

## Number of questions
The current Spark exam consists of 7 hands-on tasks and you need to get 4 correct to pass. A task either right or wrong - there is no partial credit. If you need experience with Spark on HDP, I recommend candidates work through the Spark tutorials on the Hortonworks Sandbox.

## Regarding the environment of the test, here are the versions:
HDP 2.4.0
Spark 1.6
Scala 2.10.5
Python 2.7.6 (pyspark)
You will submit the code in the spark (or python) shell. You can either write a script to a file and then execute the file or type directly on the shell. I hope that this helps.

#################################################################### HDPCD-SPARK : Syllabus ###########################

Core Spark
===========
Initialize a Spark application
Run a Spark job on YARN
Create an RDD
Create an RDD from a file or directory in HDFS
* Persist an RDD in memory or on disk: 
	1. http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence
	2. https://www.youtube.com/watch?v=PB9X-IyzwQk
	3. https://www.youtube.com/watch?v=dPHrykZL8Cg
	4. http://sujee.net/2015/01/22/understanding-spark-caching/#.WOlaZtzyuM8
	
Perform Spark transformations on an RDD
Perform Spark actions on an RDD
*Create and use broadcast variables and accumulators (Shared Variables)
*Configure Spark properties
*Write a Spark Core application in Python or Scala
** case class, ORC, 


Spark SQL
===========
*Perform operations on a DataFrame
*Write a Spark SQL application
*Use Hive with ORC from Spark SQL
*Write a Spark SQL application that reads and writes data from Hive tables
*Create Spark DataFrames from an existing RDD

ITVersity Learning plan: http://discuss.itversity.com/c/certifications/hdpcd-spark
=======================
Step1: Setup Hadoop and Spark Environment as well as IDE
Learn programming using Scala so that we can build spark based projects using Scala as programming language.

Step2: Prepare for HDPCD Spark using Scala as programming language.
	a. REPL & Basic programming constructs
	b. Object Oriented Concepts
	c. Collections and functional programming on collections
	d. Creating sbt projects using Eclipse or Intellij
	
Step3: Prepare for HDPCD Spark (watch plalist 4th to 9th)
	a. Getting started with Spark
	b. Develop word count program
	c. Execute it on the cluster
	d. Understanding HDFS briefly
	e. Develop Spark program to compute daily revenue
	f. Following are different transformations and actions covered
	g. map, flatMap, filter
	h. groupByKey, reduceByKey, aggregateByKey
	i. actions - collect, take
	j. Programs are developed using Intellij and built using sbt	
	
Step4: prepare for HDPCD Spark - Continued (Watch playlist 10th to 13th)
	a. Sorting and Ranking using Spark APIs
		1. top
		2. sortByKey
		3. takeOrdered
		4. groupByKey
	b. Reading and writing data using different file formats
	c. Accumulators and Broadcast Variables	
	

---
3. Core Spark
	a. Spark Architecture
	b. Spark Configuration and Spark Context
	c. Resilient Distributed Datasets
	d. Transformations
	e. Actions
4. Data Frames and Spark SQL
	a. Data Frames and Data Frame Operations
	b. Spark native SQL
	c. Spark Hive SQL	
	




#########################################################################################################################################
#################################################################### TIPS  - General  ##############################

# Login:
$ spark-shell
$ spark-shell --conf spark.ui.port=22222 --master yarn-client
$ spark-shell --conf spark.ui.port=22322 --master yarn-client
$ spark-shell --conf "spark.ui.port=10101" --master yarn-client
	# You can change the port no as you like.
$ pyspark

# How to check the current Mode:
scala> sc.master
res1: String = yarn-client
	# Spark indicates that you are in yarn mode

scala> sc.master
res1: String = local
	# Spark indicates that you are in local mode

Execution: Here 10 is the parameter
$ spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 1 \
    --queue thequeue \
    lib/spark-examples*.jar \
    10
	
## Execution with Kerberos in k12
# Here spark.hadoop.fs.hdfs.impl.disable.cache added to fix on of the issue but not fixed
$ spark-submit --conf spark.executor.extraClassPath="/opt/cloudera/parcels/CDH-5.7.0-1.cdh5.7.0.p1464.1349/
				--master yarn \
				--deploy-mode cluster \
				--queue realtime_queue \
				--conf spark.hadoop.fs.hdfs.impl.disable.cache=ture \ 
				--conf spark.yarn.maxAppAttempts=4 \
				--conf spark.yarn.am.attemptFailuresValidityInterval=1h \
				--files /home/oracle/workspace/adaptive/bda-config-files/recommender_activitiyvisitevent_stre... \
				--principal oracle@BDA.K12.COM \
				--keytab /home/oracle/auth/oracle.keytab \
				--class com.k12.nextgen.consumer.ActivityVisitEventConsumer /home/oracle/workspace/adaptive/

ITVERSITY GistGithub: https://gist.github.com/dgadiraju
	
1. In Spark, it is not mandatory to mention the data type. But we need to mention whether it is Mutable/Immutable (Var/Val). 
2. In Spark, In general, we use more Immutable (never change)	
3. Once we applied Transformations, it will not be exeucted immediately, It just build the flow and execution only if action applied. This is called Lazy evalution
4. RDD is nothing but a collection. Each line/record will be considered as an item
5. We use LAMBDA for Transformations
6. cat /etc/hadoop/conf/core-site.xml   	## To get the NameSpace/Fully qualified path
7. ls  /etc/spark/conf						## Spark Configuration Path
8. cat /etc/spark/conf/spark-defaults.conf	## Spark default Configuration 
9. To access hive tables, you should have hive-site.xml file, or soft link on Spark configuration path (/etc/spark/conf/hive-site.xml)
10. To write code, use Object, Class, Function 
11. setMaster in SparkConf can be used to run the application in YARN, Local or Mesos Mode 
	Scala : val conf = new SparkConf().steAppName("Test1").setMaster("local") 
	Python: conf = SparkConf().setMaster("local").setAppName("WineQuality")
12. Installing Mesos: Spark 1.6.2 is designed for use with Mesos 0.21.0 and does not require any special patches of Mesos.
13. What is local[4] in, spark-submit --class "SimpleApp" --master local[4] target/scala-2.10/simple-project_2.10-1.0.jar
	Ans: To run locally with one thread by default, or local[N] to run locally with N threads
	val conf = new SparkConf().setAppName("test").setMaster("local[*]")
	local[*] Run Spark locally with as many worker threads as logical cores on your machine.
14. How to Check 'sbt' availability? Run the following command: $ sbt 
15. While executing "sbt package" command getting the following error: <<refer Practice_Spark_Scala_QR.docx - Known Issue>>
	a. object apache is not a member of package org sbt
	b. object sql is not a member of package org.apache.spark
	c. object hive is not a member of package org.apache.spark.sql
17. Frequently Used plugins in SBT
	# For Spark core
	libraryDependencies += "org.apache.spark" %% "spark-core" % "1.6.2"
	# To pass the arguments from a file, NOTE the version for typesafe
	libraryDependencies += "com.typesafe" % "config" % "1.3.0"
	# For SQL, DATAFRAMES
	libraryDependencies += "org.apache.spark" %% "spark-sql" % "1.6.2"
	# For Hive
	libraryDependencies += "org.apache.spark" %% "spark-hive" % "1.6.2"

17.1 To reflect the SBT package in Eclipse (scala interpreter), 
	run $ sbt eclipse
	https://www.youtube.com/watch?v=kPunHXbDy18&index=63&list=PLf0swTFhTI8pgc4wyhT8lIEJCPT08RI3X	
	
18. Frequently Used packages to Import
		import org.apache.spark.SparkConf
		import org.apache.spark.SparkContext
		import org.apache.spark.sql.hive.HiveContext
		import org.apache.spark.sql.hive.orc._
		import org.apache.spark.sql._
		
		# DataFrames:??
		import org.apache.spark.sql._
		import org.apache.spark.sql.functions._ 
		import sqlContext.implicits._ # To be applied after declare: val sqlContext = new SQLContext(sc)

		# To check the hadoop path exists
		import org.apache.hadoop.fs._
		val fs = FileSystem.get(sc.hadoopConfiguration)
		val inputPathExists = fs.exists(new Path(inputPath))

		# RDD Persistence (or) Cache (MEMORY_ONLY, MEMORY_ONLY_SER, etc)
		import org.apache.spark.storage.StorageLevel._
		http://sujee.net/2015/01/22/understanding-spark-caching/#.WOlaZtzyuM8
		
		# To Pass the parameter
		import com.typesafe.config._
		
		# Using Hive with ORC 
		import org.apache.spark.sql.hive.orc._
		import org.apache.spark.sql._
		
		
19. Creating Hive Context
		Option1:
		import org.apache.spark.SparkConf
		import org.apache.spark.SparkContext
		import org.apache.spark.sql.hive.HiveContext

		val conf = new SparkConf().setAppName("Spark Connect to Hive - emp table")
		val sc = new SparkContext(conf)
		val sqlContext = new HiveContext(sc)
		
		Option2:
		import org.apache.spark.SparkConf
		import org.apache.spark.SparkContext
		import org.apache.spark.sql._

		val conf = new SparkConf().setAppName("Spark Connect to Hive - emp table")
		val sc = new SparkContext(conf)
		val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
		
20. how to read a csv file? Ans: Use textFile on sparkcontext as follows
		val yahoo_stocks = sc.textFile("hdfs://sandbox.hortonworks.com:8020/tmp/yahoo_stocks.csv")	

21. RDD vs DATAFRAME
		a. RDD uses SparkContext (sc) 	  
			Ex: 
			val dataRDD = sc.textFile("user/saranvisa/scala-sbt-input/empid.txt")
			dataRDD.collect().foreach(println)
			
		b. DATAFRAME uses sqlContext. It is nothing but Collection of data organized into "named Columns" then use sql on top of it
			Ex: 
			import org.apache.spark.sql.functions._ 
			val df = sqlContext.jsonFile("people.json")
			val df = sqlContext.sql("select * from qa.emp")
			df: org.apache.spark.sql.DataFrame = [empid: int, empname: string]
			
			df.show
			df.select(df("name"), df("age") + 1).show()    # To increase the column value
			
21.1: DATAFRAME 
	 a. is nothing but Collection of data organized into "named Columns" then use sql type of query on top of it
	 b. This concept is not new, already some languages like pythong using this, it has been inheritated into spark
	 c. We can create DATAFRAME on the raw data on the fly and use SQL Type of query
	 d. Need to create CASE CLASS to use the DATAFRAME. NOTE: Understand the diff between normal class & case class

22. Frequently Used File formats
		a. textFile
		b. jsonFile
		c. sequenceFile
		d. objectFile
		e. hadoopFile		## For OLD API
		f. newAPIHadoopFile	## For NEW API
		
23. Hadoop has two APIs (OLD & NEW): 
		a. OLD API : org.apache.hadoop.mapred
		b. NEW API : org.apache.hadoop.mapreduce
		
		14th min of https://www.youtube.com/watch?v=UPMGipRS_-E&list=PLf0swTFhTI8pgc4wyhT8lIEJCPT08RI3X&index=40
		http://hadoopbeforestarting.blogspot.com/2012/12/difference-between-hadoop-old-api-and.html

24. Create Tuple (KEY,VALUE) using map in order to JOIN two RDDs
		val dataOrderTUP 	= dataOrderCMP.map(rec => (rec.split(",")(0).toInt,rec.split(",")(1)))
		val orderItemsTUP 	= orderItems.map(rec => (rec.split(",")(1).toInt,rec.split(",")(4).toFloat))
		val orderJOIN 		= orderItemsTUP.join(dataOrderTUP)
		
25. sortByKey Can be applied on Tuple (Key, Value)		
26.	groupByKey is more generic function and it does not use combiner. It is more appropriate to use for complex group transformations such as sorting
    and ranking. 
		a. It does not use Combiner & Reducer logic	
		b. It is Suitable for sorting and ranking
27. reduceByKey is more appropriate function to aggregate the data for current scenario used in the below usecases. 
		a. It uses Combiner & Reducer logic
		b. It is suitable when both Combiner & Reducer logic are same. It means Input value & Outoupt Value are same
28. aggregateByKey is used in the scenarios where we have different logic for combiner and reducer
		a. It uses Combiner & Reducer logic
		b. It is suitable when both Combiner & Reducer logic are different. It means Input value & Outoupt Value are different
		c. https://www.youtube.com/watch?v=xh3uEop3nvI&index=50&list=PLf0swTFhTI8pgc4wyhT8lIEJCPT08RI3X

29. Supporting Join Types in Tuple
	a.	join
	b.	leftOuterJoin
	c.	rightOuterJoin
	d.	fullOuterJoin
30. 


##############################################################################   SPARK  INTRODUCTION ##################################

SPARK INTRODUCTION
1. Create RDD for Parallelized Collections
2. Cluster Mode
3. Different Modules in spark
4. Create a Simple Spark Program
5. About Spark Conf
6. About Spark Context
7. 



########################################################   RDD - Resilient Distributed Datasets ##################################

## There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat

# First Way:
val data = Array(1, 2, 3, 4, 5)
val dataRDD = sc.parallelize(data)

# Second Way:
val dataRDD = sc.textFile("hdfs://nn01.itversity.com:8020/user/saranvisa/scala-sbt-input/empid.txt")

# Note: use textFile to read CSV too
val yahoo_stocks = sc.textFile("hdfs://sandbox.hortonworks.com:8020/tmp/yahoo_stocks.csv")

## Spark based collection:
1. Array
val data = Array(1,2,3,4,5)
2. List
val data = List(1,2,3,4,5)

## Spark based Collections:
3. RDD on parallelize Collections
scala> val dataRDD = sc.parallelize(data)		## Use the above Array/List called data to create dataRDD

scala> dataRDD.reduce((acc,value) => acc+value) ## Use reduce to sum the value 
res4: Int = 15

## Created a file in local called empid.txt with 6 records
1000
1001
1002
1003
1004
1005


## To separate the data into a new RDD where we do not have the header above and :
val yahoo_stocks = sc.textFile("hdfs://sandbox.hortonworks.com:8020/tmp/yahoo_stocks.csv")
val data = yahoo_stocks.mapPartitionsWithIndex { (idx, iter) => if (idx == 0) iter.drop(1) else iter }


## READ LOCAL DATA: Create RDD for external data sets on "OS/local" file system
## Note: If using a path on the "local filesystem", the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system
scala> val dataRDD = sc.textFile("file:///home/saranvisa/empid.txt")
dataRDD: org.apache.spark.rdd.RDD[String] = file:///home/saranvisa/empid.txt MapPartitionsRDD[8] at textFile at <console>:27
## NOTE: refer the above line, All the records will be considered as RDD[String], so for Numeric calculation, you need to cast it

scala> dataRDD.count()
res7: Long = 6

scala> dataRDD.reduce((acc,value) => acc+value)
res9: String = 100010011002100310041005

scala> dataRDD.map(rec=>rec.toInt).reduce((acc,value) => acc+value)
res8: Int = 6015

## READ HDFS DATA: Create RDD for the data from HDFS
scala>val dataRDD = sc.textFile("hdfs://nn01.itversity.com:8020/user/saranvisa/scala-sbt-input/empid.txt")

#empid.txt has value as follows
1000
1001
1002
1003
1004
1005

scala> dataRDD.reduce((a,b) => a+b) 
res1: String = 100010011002100310041005  	## Result before cast to Int

scala> dataRDD.map(x=>x.toInt).reduce((a,b)=> a+b)
res3: Int = 6015							## Result after cast to Int



###########################################################  Cluster Modes in Spark ##################################

## Different Cluster Modes: 
# For Development
1. Local Mode: For Development. Only One JVM will be created, it will take care of exeucting end-to-end Spark
2. Standalone: We can use it to troubleshoot some issues

# For Production
3. YARN: It will be little bit slower. It is also has some advantages like divide the work between MapReduce and Spark
4. Mesos: If you have only Spark and not using MapReduce then Mesos is the right option

##YARN
./bin/spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 1 \
    --queue thequeue \
    lib/spark-examples*.jar \
    10
	
# To launch a Spark application in client mode, do the same, but replace cluster with client. The following shows how you can run spark-shell in client mode:
$ ./bin/spark-shell --master yarn --deploy-mode client

## Mesos
val conf = new SparkConf().setMaster("mesos://HOST:5050").setAppName("My app").set("spark.executor.uri", "<path to spark-1.6.2.tar.gz uploaded above>")
val sc = new SparkContext(conf)

./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master mesos://207.184.161.138:7077 \
  --deploy-mode cluster
  --supervise
  --executor-memory 20G \
  --total-executor-cores 100 \
  http://path/to/examples.jar \
  1000

########################################################   Different Modules in Spark ##################################

## Different Modules
1. Core Spark 					## For Transformations & Actions
2. Data Frames, Data Sets, SQL: ## To define structure to Data. Integration of RDD with SQL by creating structure at run time. 
3. Spark Streaming				## It takes data from other sources like Flume/Kafka and process as part of analysis and store in target
4. MLLib						## Machine Learning Libraries algorithm
5. GraphX						## for Graph
6. Bagel						## Based on Google, for Graph. Either use GraphX or Bagel
7. Spark-R						## New option in Spark


########################################################   Create a simple Spark Program ##################################

import org.apache.spark.SparkContext
import org.apache.spark.SparkConf

scala> object SimpleApp {
     | def main(args:Array[String]) {
     | val conf = new SparkConf().setAppName("scaka spark -kum").setMaster(args(0))
     | val sc   = new SparkContext(conf)
     | val vList= List(1,2,3,4,5)
     | val dataRDD = sc.parallelize(vList)
     | dataRDD.saveAsTextFile(args(1))
     | }
     | }
defined module SimpleApp

########################################################   About Spark Conf ##################################

## Spark Configuration Path: cat /etc/spark/conf/spark-defaults.conf

import org.apache.spark.SparkConf		
val conf = new SparkConf().setAppName("My First Program").setMaster("local")
for (i <- conf.getAll) {
	println(i)
}

Result
spark.master,local
spark.appname,My First Program

########################################################   About Spark Context ##################################

## Spark Configuration Path: cat /etc/spark/conf/spark-defaults.conf

# Different Contexts
1. local
2. yarn-client   ## Refer above, Cluster modes
3. Mesos-URL
4. Spark-URL (Spark standalone)

import org.apache.spark.SparkConf		
import org.apache.spark.SparkContext
val conf = new SparkConf().setAppName("My First Program").setMaster("local")
val conf = new SparkConf().setAppName("My First Program").setMaster("yarn-client")
val conf = new SparkConf().setAppName("My First Program").setMaster("mesos-URL")
val conf = new SparkConf().setAppName("My First Program").setMaster("spark-URL")
val conf = new SparkConf().setAppName("My First Program").setMaster(args(0))		## To pass the value during runtime
val conf = new SparkConf().setMaster("mesos://HOST:5050").setAppName("My app").set("spark.executor.uri", "<path to spark-1.6.2.tar.gz uploaded above>")
val sc = new SparkContext(conf)

########################################################   About Actions  ##################################

scala> val dataRDD = sc.textFile("/user/saranvisa/scala-sbt-input/empid_name.txt")
dataRDD: org.apache.spark.rdd.RDD[String] = /user/saranvisa/scala-sbt-input/empid_name.txt MapPartitionsRDD[11] at textFile at <console>:27

scala> dataRDD.count()						## Count
res8: Long = 7

scala> dataRDD.collect()					## Collect
res11: Array[String] = Array(1000,kumar, 1001,Raj, 1003,Adam, 1004,Alex, 1005,Jose, 1006,kumar, 1007,Alex)

scala> dataRDD.first()						## first used to take the header
res12: String = 1000,kumar

scala> dataRDD.take(3)						## take
res13: Array[String] = Array(1000,kumar, 1001,Raj, 1003,Adam)

scala> dataRDD.collect().foreach(println)	## foreach on collect
1000,kumar
1001,Raj
1003,Adam
1004,Alex
1005,Jose
1006,kumar
1007,Alex

scala>  dataRDD.cache()						## Cache
res33: dataRDD.type = /user/saranvisa/scala-sbt-input/empid_name.txt MapPartitionsRDD[38] at textFile at <console>:28


########################################################   About Transformations  ############################

### Transformation used to filter the data
scala> val dataRDD = sc.textFile("/user/saranvisa/scala-sbt-input/empid_name.txt")
dataRDD: org.apache.spark.rdd.RDD[String] = /user/saranvisa/scala-sbt-input/empid_name.txt MapPartitionsRDD[11] at textFile at <console>:27

scala> dataRDD.collect().foreach(println)
1000,kumar
1001,Raj
1003,Adam
1004,Alex
1005,Jose
1006,kumar
1007,Alex

## filter using Contains
scala> dataRDD.filter(x => x.contains("kumar")).foreach(println)	
1000,kumar
1006,kumar

## filter using "=="
scala> dataRDD.filter(x => x.split(",")(1) == "kumar").foreach(println) 
1006,kumar
1000,kumar

## filter the header using mapPartitionsWithIndex
val noheaderRDD = dataRDD.mapPartitionsWithIndex((idx,iter) => if (idx==0) iter.drop(1) else iter )	

scala> noheaderRDD.collect().foreach(println)
1001,Raj
1003,Adam
1004,Alex
1005,Jose
1006,kumar
1007,Alex



########################################################   UseCase1: Simple App  ##################################

https://spark.apache.org/docs/1.6.2/quick-start.html

$ mkdir SimpleProject
$ cd SimpleProject/

# vi simple.sbt		
name := "Simple Project"
version := "1.0"
scalaVersion := "2.10.5"
libraryDependencies += "org.apache.spark" %% "spark-core" % "1.6.2"




$ mkdir -p src/main/scala
$ cd src/main/scala

# vi SimpleApp.scala:
/* SimpleApp.scala */
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = "/user/saranvisa/scala-sbt-input/empid_name.txt"
    val conf = new SparkConf().setAppName("Simple Application")
    val sc = new SparkContext(conf)
    val logData = sc.textFile(logFile, 2).cache()
    val numAs = logData.filter(line => line.contains("a")).count()
    val numBs = logData.filter(line => line.contains("b")).count()
    println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
  }
}

$ cd ../../..  # back to SimpleApp directory

$ sbt package  # jar will be created under target/scala-2.10

$ spark-submit \
> --class SimpleApp \
> --master local \
> target/scala-2.10/simple-project_2.10-1.0.jar

# Output
Multiple versions of Spark are installed but SPARK_MAJOR_VERSION is not set
Spark1 will be picked by default
Lines with a: 4, Lines with b: 0



######################################   UseCase2: Word Count (Pass parameters for input/output)  ##################################

## CODE in COMMAND LINE 
scala> val dataRDD = sc.textFile("/user/saranvisa/scala-sbt-input/empid_name.txt")
dataRDD: org.apache.spark.rdd.RDD[String] = /user/saranvisa/scala-sbt-input/empid_name.txt MapPartitionsRDD[11] at textFile at <console>:27

scala> val wcRDD = dataRDD.flatMap(x => x.split(",")).map(x => (x,1)).reduceByKey((x,y) => x+y)
wcRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[19] at reduceByKey at <console>:29

scala> wcRDD.collect().foreach(println)
(1005,1)
(1001,1)
(Alex,2)
(kumar,2)
(1003,1)
(1007,1)
(Adam,1)
(Raj,1)
(1000,1)
(1004,1)
(1006,1)
(Jose,1)

## CODE in SHELL
# Make sure sbt has already been installed by running command $ sbt

$ mkdir WordCount
$ cd WordCount/

## vi wordcount.sbt
name := "Word Count"
version := "1.0"
scalaVersion := "2.10.5"
libraryDependencies += "org.apache.spark" %% "spark-core" % "1.6.2"
libraryDependencies += "com.typesafe" % "config" % "1.3.0"

$ mkdir -p src/main/scala
$ cd src/main/scala
 
# vi WordCount.scala

package wordCount

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

object wrdCount{
	def main (args:Array[String]) {
		val conf = new SparkConf().setAppName("word count")
		val sc = new SparkContext(conf)
		val inputPath = args(0)
		val outputPath = args(1)
		
		## inputPath = /user/saranvisa/scala-sbt-input/word_count.txt
		val dataRDD = sc.textFile(inputPath)
		val wc = dataRDD.flatMap(x=> x.split(",")).map(x=>(x,1)).reduceByKey((x,y)=>x+y)
		wc.collect().foreach(println)
	}
}

$ cd ../../..  # back to WordCount directory

$ sbt package  # jar will be created under target/scala-2.10

$ spark-submit \
--class wrdCount \
--master local \
target/scala-2.10/word-count_2.10-1.0.jar

# Output:
Spark1 will be picked by default
(1005,1)
(Adam,1)
(1001,1)
(Raj,1)
(1000,1)
(Alex,2)
(kumar,2)
(1004,1)
(1006,1)
(1003,1)
(1007,1)
(Jose,1)



#####################################   UseCase3: find the line with the most words  ##################################

## ReduceByKey vs Reduce
a.	ReduceByKey is a Transformation, like groupby 
b.	Reduce is an Action. To sum the value

scala> val dataRDD = sc.textFile("/user/saranvisa/scala-sbt-input/words1.txt")
dataRDD: org.apache.spark.rdd.RDD[String] = /user/saranvisa/scala-sbt-input/words1.txt MapPartitionsRDD[25] at textFile at <console>:27

scala> dataRDD.collect().foreach(println) ## 5th line has most words
one
two words
there are three
four words are available
you can find five words

## Method 1: 
scala> dataRDD.map(x => x.split(" ").size).reduce((a,b) => if (a>b) a else b)
res26: Int = 5

## Method 2: Using Math
scala> import java.lang.Math
import java.lang.Math

scala> dataRDD.map(x=> x.split(" ").size).reduce((a,b) => Math.max(a,b))
res27: Int = 5


###########################   UseCase4: Using Transformations (map, reduceByKey), Action, List, Array, etc  #######################
## map
scala> val lst = List(1,2,3,4)
lst: List[Int] = List(1, 2, 3, 4)

scala> lst.map(x=> x*2)
res5: List[Int] = List(2, 4, 6, 8)


###########################   UseCase5: Sequence File (KEY, VALUE)  ###############################################################

1. Cannot read a text file as sequence file, so read a text file and write it into to sequence file then read it as sequence file. 
2. Pre-request:  that the source should be already a sequence file

scala> import org.apache.spark.SparkContext, org.apache.spark.SparkConf
scala> import org.apache.hadoop.mapreduce.lib.output
scala> import org.apache.hadoop.mapreduce.lib.output._
scala> import org.apache.hadoop.io._

scala> val dataRDD = sc.textFile("/user/saranvisa/scala-sbt-input/empid_name.txt")

## If you don't know the KEY column, then consider NULL as key
scala> dataRDD.map(rec => (NullWritable.get(),rec)).foreach(println)
((null),1005,Jose)
((null),1000,kumar)
((null),1006,kumar)
((null),1001,Raj)
((null),1007,Alex)
((null),1003,Adam)
((null),1004,Alex)

## If you know the KEY column
scala> dataRDD.map(rec => (rec.split(",")(0),rec.split(",")(1))).foreach(println)
(1005,Jose)
(1006,kumar)
(1000,kumar)
(1007,Alex)
(1001,Raj)
(1003,Adam)
(1004,Alex)

## To Write a file as Sequence File
scala> dataRDD.map(rec => (rec.split(",")(0),rec.split(",")(1))).saveAsSequenceFile("/user/saranvisa/scala-sbt-input/empid_name_seq")

$ hdfs dfs -ls /user/saranvisa/scala-sbt-input/empid_name_seq

## To Read a sequence file
scala> val path = "/user/saranvisa/scala-sbt-input/empid_name_seq"
scala> sc.sequenceFile(path,classOf[IntWritable],classOf[Text]).map(rec => rec.toString()).collect().foreach(println)
(1000,kumar)
(1001,Raj)
(1003,Adam)
(1004,Alex)
(1005,Jose)
(1006,kumar)
(1007,Alex)

		

#######################################  UseCase5: JSON file        ############################################################## 		
		
#######################################  UseCase6: Invoke a function ############################################################## 		

scala> def sum(a:Int, b:Int):Int=a+b
sum: (a: Int, b: Int)Int

scala> sum(3,5)
res6: Int = 8

#######################################  UseCase7: Spark to Hive connect (emp table) ############################################### 		

## CODE in SHELL
# Make sure sbt has already been installed by running command $ sbt

$ mkdir HiveEmp
$ cd HiveEmp/

## vi hiveemp.sbt
name := "Hive Emp"
version := "1.0"
scalaVersion := "2.10.5"
libraryDependencies += "org.apache.spark" %% "spark-core" % "1.6.2"
libraryDependencies += "org.apache.spark" %% "spark-sql" % "1.6.2"
libraryDependencies += "org.apache.spark" %% "spark-hive" % "1.6.2"


$ mkdir -p src/main/scala
$ cd src/main/scala
 
$ vi HiveEmp.scala
object HiveEmp {
   def main(args:Array[String]) {

      import org.apache.spark.SparkConf
      import org.apache.spark.SparkContext
      import org.apache.spark.sql.hive.HiveContext

      val conf = new SparkConf().setAppName("Spark Connect to Hive - emp table")
      val sc = new SparkContext(conf)
      val sqlContext = new HiveContext(sc)

      val dataRDD = sqlContext.sql("select * from qa.emp")
      dataRDD.collect().foreach(println)

 }
}

$ cd ../../..  # back to HiveEmp directory

$ sbt package  # jar will be created under target/scala-2.10

$ spark-submit \
> --class HiveEmp \
> --master local \
> target/scala-2.10/hive-emp_2.10-1.0.jar
Multiple versions of Spark are installed but SPARK_MAJOR_VERSION is not set
Spark1 will be picked by default
[10,kumar]
[20,James]
[30,Sam]
[40,Dinesh]
[50,Diya]
[60,Sachein]
[70,Sumi]

## Hive version
hive> select * from qa.emp;
OK
10      kumar
20      James
30      Sam
40      Dinesh
50      Diya
60      Sachein
70      Sumi
Time taken: 1.835 seconds, Fetched: 7 row(s)


#######################################  UseCase8: To remove header ############################################################## 		

scala> val dataRDD = sc.textFile("/user/saranvisa/scala-sbt-input/empid_name_header.txt")
dataRDD: org.apache.spark.rdd.RDD[String] = /user/saranvisa/scala-sbt-input/empid_name_header.txt MapPartitionsRDD[1] at textFile at <console>:30

scala> dataRDD.collect().foreach(println)
id,name
1000,kumar
1001,Raj
1003,Adam
1004,Alex
1005,Jose
1006,kumar
1007,Alex

scala> val noheaderRDD = dataRDD.mapPartitionsWithIndex((idx,iter) => if (idx==0) iter.drop(1) else iter )
noheaderRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at mapPartitionsWithIndex at <console>:32

scala> noheaderRDD.collect().foreach(println)
1000,kumar
1001,Raj
1003,Adam
1004,Alex
1005,Jose
1006,kumar
1007,Alex


#######################################  UseCase10: ORDERS & ORDER_ITEMS #####################################################
orders1
1,2013-11-09 00:00:00.0,8326,COMPLETE
2,2013-11-09 00:00:00.0,5729,COMPLETE
3,2013-11-09 00:00:00.0,1870,CLOSED
4,2013-11-09 00:00:00.0,2366,PENDING
5,2013-11-10 00:00:00.0,5723,COMPLETE

order_items1
1,1,502,3,150.3,50.1
2,1,503,2,22.6,11.3
3,3,504,1,30.1,30.1
4,4,505,1,50.2,50.2
5,5,506,2,20.2,10.1
6,5,507,3,30.3,10.1

scala> val dataOrder = sc.textFile("/user/saranvisa/scala-sbt-input/orders1")
scala> val dataOrderCMP = dataOrder.filter(rec => rec.split(",")(3) == "COMPLETE")
dataOrderCMP: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[12] at filter at <console>:29

scala> dataOrderCMP.foreach(println)
5,2013-11-10 00:00:00.0,5723,COMPLETE
1,2013-11-09 00:00:00.0,8326,COMPLETE
2,2013-11-09 00:00:00.0,5729,COMPLETE

## Create Tuple (KEY,VALUE) from order on COMPLETE orders where KEY=order_id and VALUE=order_date. This will help to JOIN
scala> val dataOrderTUP = dataOrderCMP.map(rec => (rec.split(",")(0).toInt,rec.split(",")(1)))
dataOrderTUP: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[8] at map at <console>:31

scala> dataOrderTUP.foreach(println)
(1,2013-11-09 00:00:00.0)
(2,2013-11-09 00:00:00.0)
(5,2013-11-10 00:00:00.0)

scala> val orderItems = sc.textFile("/user/saranvisa/scala-sbt-input/order_items1")

## Create a Tuple (KEY,VALUE) from Order_items where KEY=order_id and VALUE=order_item_subtotal
scala> val orderItemsTUP = orderItems.map(rec => (rec.split(",")(1).toInt,rec.split(",")(4).toFloat))
orderItemsTUP: org.apache.spark.rdd.RDD[(Int, Float)] = MapPartitionsRDD[7] at map at <console>:29

scala> val orderItemsTUP1 = orderItemsTUP.reduceByKey((acc,value) => (acc+value))
orderItemsTUP1: org.apache.spark.rdd.RDD[(Int, Float)] = ShuffledRDD[9] at reduceByKey at <console>:31

## Result after reduceByKey
scala> orderItemsTUP1.foreach(println)
(4,50.2)
(1,172.90001)
(3,30.1)
(5,50.5)

scala> val orderJOIN = orderItemsTUP1.join(dataOrderTUP)
orderJOIN: org.apache.spark.rdd.RDD[(Int, (Float, String))] = MapPartitionsRDD[16] at join at <console>:39

scala> orderJOIN.foreach(println)
(1,(172.90001,2013-11-09 00:00:00.0))
(5,(50.5,2013-11-10 00:00:00.0))


## Discard order_id and get only date & sub-total
scala> val orderJOINmap = orderJOIN.map(rec => (rec._2._1, rec._2._2))

#######################################  UseCase11: TUPLE & UNDERSCORE NOTATIONs #####################################################

## Nested Tuple. Because VALUE has another Tuple
scala> orderJOIN1.collect().foreach(println)
(1,(150.3,2013-11-09 00:00:00.0))
(1,(22.6,2013-11-09 00:00:00.0))
(5,(20.2,2013-11-10 00:00:00.0))
(5,(30.3,2013-11-10 00:00:00.0))

## Use Underscore Notation to get values from Tuple
scala> orderJOIN1.filter(rec => rec._1 ==1).collect().foreach(println)
(1,(150.3,2013-11-09 00:00:00.0))
(1,(22.6,2013-11-09 00:00:00.0))

## KEY = _1 , VALUE = _2._1 and _2._2
scala> orderJOIN1.map(rec => rec._1.toString + ',' +  rec._2._1.toString + ',' + rec._2._2.toString).collect().foreach(println)
1,150.3,2013-11-09 00:00:00.0
1,22.6,2013-11-09 00:00:00.0
5,20.2,2013-11-10 00:00:00.0
5,30.3,2013-11-10 00:00:00.0

#######################################  UseCase12: sortByKey #####################################################

# Can be applied on Tuple (Key, Value)
scala> orderJOIN1.sortByKey().collect().foreach(println)
(1,(150.3,2013-11-09 00:00:00.0))
(1,(22.6,2013-11-09 00:00:00.0))
(5,(20.2,2013-11-10 00:00:00.0))
(5,(30.3,2013-11-10 00:00:00.0))

scala> orderJOIN1.sortByKey(false).collect().foreach(println)
(5,(20.2,2013-11-10 00:00:00.0))
(5,(30.3,2013-11-10 00:00:00.0))
(1,(150.3,2013-11-09 00:00:00.0))
(1,(22.6,2013-11-09 00:00:00.0))


#####################################  UseCase13: SHARED VARIABLES: BROADCAST & ACCUMULATOR  ##########################

## Create and use broadcast variables and accumulators (Shared Variables)

## ABOUT BROADCAST:
1. This is an API function on SparkContext(sc), it is a Shared variable
2. Pass RDD as parameter, collectAsMap is an API 
val bv = sc.broadcast(productDepartmentsMap.collectAsMap())
3. Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.

4. To get value from broadcast variable. Ex: Consider bv is a boradcast variable then $ bv.value (or) $println(bv)
5. Broadcast variables are immutable, that means, BV is readonly, we cannot change the value in BV

## Practical-1:
# Broadcast variables are created from a variable v by calling SparkContext.broadcast(T, scala.reflect.ClassTag<T>). The broadcast variable is a wrapper around v, and its value can be accessed by calling the value method. The interpreter session below shows this:

 scala> val broadcastVar = sc.broadcast(Array(1, 2, 3))
 broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int} = Broadcast(0)

 scala> broadcastVar.value
 res0: Array[Int] = Array(1, 2, 3)
 
# After the broadcast variable is created, it should be used instead of the value v in any functions run on the cluster so that v is not shipped to the nodes more than once. In addition, the object v should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later).
 
## Practical-2:
	# Refer "UseCase14: Join & get data from both emp & dept"
 
 
========================
## ABOUT ACCUMULATOR: 
1. This is an API function on SparkContext(sc), it is a Shared variable
2. Something similar to Counter in Java & MapReduce
3. Create an accumulator variable and use with Transformation. The normal variable will NOT work as accumulator
Ex:
scala> val accum = sc.accumulator(0, "My Accumulator")  			## 0 is an initial value
scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)
4. Accumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, their value is only updated once that RDD is computed as part of an action
5. To get value from accumulator variable. Ex: Consider accum is an accumulator variable then $ accum.value (or) $println(accum)

## Sample Usage
1. find out how many records had a valid user ID
2. How many purchases occurred within some time window

## Issues with accumulators: Here are some of the known issues with accumulators
1. Unless tasks are executed you will not see details about counters
2. Spark guarantees accumulators to be updated only in first execution
3. If any task is re-executed the results can be inconsistent
4. The issue is prevalent in both transformations and actions


## Practical-1
	val data = Array(1, 2, 3, 4, 5)
	val accum = sc.accumulator(0, "Accumulator value")	## Mention a name for accumulator
	val dataRDD = sc.parallelize(data).foreach(x => accum += x)

	scala> accum.value
	res2: Int = 10
	<or>
	scala> println(accum)
	10


## Practical-2

    val ordersRDD = sc.textFile("/user/saranvisa/scala-sbt-input/orders")
	val orderItemsRDD = sc.textFile("/user/saranvisa/scala-sbt-input/order_items")

	# Filter WITHOUT ACCUMULATOR
    val ordersCompletedBfr = ordersRDD.filter(rec => (rec.split(",")(3) == "COMPLETE"))
	
	# Filter WITH ACCUMULATOR
	val ordersAllAccum = sc.accumulator(0,"Orders All count")
	val ordersCompleteAccum = sc.accumulator(0,"Orders Completed Accumulator")
	val orderCompleteAft = ordersRDD.filter(rec=> { 
	ordersAllAccum += 1
	if (rec.split(",")(3) == "COMPLETE")
	{
	ordersCompleteAccum += 1
	}
	(rec.split(",")(3) == "COMPLETE") 
	})

	# The resul will be 0 until you apply action on RDD. Because it is lazy evaluation
	scala> println(ordersAllAccum)
	0
	scala> println(ordersCompleteAccum)
	0
	
	# Action applied on RDD
	scala> ordersCompletedAft.count()
	res7: Long = 5714

	# Accumulator returns the result now
	scala> println(ordersAllAccum)
	17221

	scala> println(ordersCompleteAccum)
	5714

## Practical-3
	# map WITHTOUT accumulator
	scala> val dataOrdTupBfr = ordersCompletedBfr.map(rec => (rec.split(",")(0).toInt, rec.split(",")(1)))
	dataOrdTupBfr: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[5] at map at <console>:35

	scala> dataOrdTupBfr.count()
	res13: Long = 5714

	scala> dataOrdTupBfr.take(2).foreach(println)
	(3,2013-07-25 00:00:00.0)
	(5,2013-07-25 00:00:00.0)

	# map WITH accumulator
	val dataOrdAccum = sc.accumulator(0,"Order count")
	val dataOrdTupAft = ordersCompletedAft.map(rec => { 
	dataOrdAccum += 1
	(rec.split(",")(0).toInt, rec.split(",")(1))
	})
	
	# before action on RDD
	scala> dataOrdAccum.value
	res12: Int = 0
	
	scala> dataOrdTupAft.count()
	res14: Long = 5714

	# after action on RDD
	scala> dataOrdAccum.value
	res15: Int = 5714

	
#######################################  UseCase14: Join & get data from both emp & dept ###################################

# Practical-1: Join emp & dept and save the output in HDFS in the below format
# Expected output: empid, empname, deptid, deptname

scala> val empRDD = sc.textFile("/user/saranvisa/scala-sbt-input/empid_name_dept.txt")
empRDD: org.apache.spark.rdd.RDD[String] = /user/saranvisa/scala-sbt-input/empid_name_dept.txt MapPartitionsRDD[3] at textFile at <console>:27

scala> empRDD.collect().foreach(println)
1000,kumar,10
1001,Raj,20
1003,Adam,30
1004,Alex,10
1005,Jose,10
1006,kumar,10
1007,Alex,40

scala> val deptRDD = sc.textFile("/user/saranvisa/scala-sbt-input/dept.txt")
deptRDD: org.apache.spark.rdd.RDD[String] = /user/saranvisa/scala-sbt-input/dept.txt MapPartitionsRDD[5] at textFile at <console>:27

scala> deptRDD.collect().foreach(println)
10,ITEngineer
20,Support
30,Manager
40,ProdSupport

scala> val jobDescRDD = sc.textFile("/user/saranvisa/scala-sbt-input/jobDesc.txt")
jobDescRDD: org.apache.spark.rdd.RDD[String] = /user/saranvisa/scala-sbt-input/jobDesc.txt MapPartitionsRDD[11] at textFile at <console>:27

scala> jobDescRDD.collect().foreach(println)
10,ITEngineer should involve in software development
20,Support engineer role is like a help desk
30,Manager will coordinate with all the team members and deliver results
40,ProdSupport people will take care of software after development


scala> val empRDDMap = empRDD.map(rec => (rec.split(",")(2), (rec.split(",")(0), rec.split(",")(1))) )
empRDDMap: org.apache.spark.rdd.RDD[(String, (String, String))] = MapPartitionsRDD[6] at map at <console>:29

scala> empRDDMap.collect().foreach(println)
(10,(1000,kumar))
(20,(1001,Raj))
(30,(1003,Adam))
(10,(1004,Alex))
(10,(1005,Jose))
(10,(1006,kumar))
(40,(1007,Alex))

scala> val deptRDDMap = deptRDD.map(rec => (rec.split(",")(0), rec.split(",")(1)))
deptRDDMap: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[7] at map at <console>:29

scala> deptRDDMap.collect().foreach(println)
(10,ITEngineer)
(20,Support)
(30,Manager)
(40,ProdSupport)


scala> val empdeptRDD = deptRDDMap.join(empRDDMap)
empdeptRDD: org.apache.spark.rdd.RDD[(String, (String, (String, String)))] = MapPartitionsRDD[37] at join at <console>:41

scala> empdeptRDD.foreach(println)
(20,(Support,(1001,Raj)))
(40,(ProdSupport,(1007,Alex)))
(30,(Manager,(1003,Adam)))
(10,(ITEngineer,(1000,kumar)))
(10,(ITEngineer,(1004,Alex)))
(10,(ITEngineer,(1005,Jose)))
(10,(ITEngineer,(1006,kumar)))


scala> val empdeptOutputRDD = empdeptRDD.map(rec => rec._2._2._1 + "," + rec._2._2._2 + "," + rec._1 + "," + rec._2._1)
empdeptOutputRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at map at <console>:37

scala> empdeptOutputRDD.collect().foreach(println)
1001,Raj,20,Support
1007,Alex,40,ProdSupport
1003,Adam,30,Manager
1000,kumar,10,ITEngineer
1004,Alex,10,ITEngineer
1005,Jose,10,ITEngineer
1006,kumar,10,ITEngineer

scala> val empdeptOutputRDD = empdeptRDD.map(rec => rec._2._2._1 + "," + rec._2._2._2 + "," + rec._1 + "," + rec._2._1).saveAsTextFile("/user/saranvisa/scala-sbt-output/emp_dept")
empdeptOutputRDD: Unit = ()
	
$ hdfs dfs -cat /user/saranvisa/scala-sbt-output/emp_dept/part*
1001,Raj,20,Support
1007,Alex,40,ProdSupport
1003,Adam,30,Manager
1000,kumar,10,ITEngineer
1004,Alex,10,ITEngineer
1005,Jose,10,ITEngineer
1006,kumar,10,ITEngineer
	
## Apply accumulator on this RDD: empdeptRDD.map(rec => rec._2._2._1 + "," + rec._2._2._2 + "," + rec._1 + "," + rec._2._1)	

val empdeptAccum = sc.accumulator(0,"Count the records")
empdeptRDD.map(rec => { empdeptAccum += 1 
			rec._2._2._1 + "," + rec._2._2._2 + "," + rec._1 + "," + rec._2._1
			}).foreach(println)
1001,Raj,20,Support
1007,Alex,40,ProdSupport
1003,Adam,30,Manager
1000,kumar,10,ITEngineer
1004,Alex,10,ITEngineer
1005,Jose,10,ITEngineer
1006,kumar,10,ITEngineer

scala> empdeptAccum.value
res9: Int = 7

## Apply broadcast variable on this RDD: empdeptRDD.collect().foreach(println)
## collectAsMap is working on RDD only after two RDD join ##
val broadcastVar = sc.broadcast(empdeptRDD.collectAsMap())

scala> broadcastVar.value
res7: scala.collection.Map[String,(String, (String, String))] = Map(30 -> (Manager,(1003,Adam)), 20 -> (Support,(1001,Raj)), 40 -> (ProdSupport,(1007,Alex)), 10 -> (ITEngineer,(1006,kumar)))

val empdeptjobDescRDD = jobDescRDD.map(rec => (rec.split(",")(0), (broadcastVar.value(rec.split(",")(0)))))


#######################################  UseCase15: RDD Persistence or Cache ##########################################

# About Persistence (or) Cache
1. Storing RDDs in memory
2. This is very useful when we use the same data repeatedly
3. It is Lazy evaluation, when we apply cache, it will not be performed until next transformation or action

	# Example for cache: https://www.youtube.com/watch?list=PL-x35fyliRwhKT-NpTKprPW1bkbdDcTTW&v=TtvxKzO9jXE
	val dataRDD = sc.textFile("/user/saranvisa/scala-sbt-input/empid_name.txt")
	
	# set RDD into Cache
	scala> dataRDD.cache()
	
	<<OR>>
	# Use cache while RDD creation
	val dataRDD = sc.textFile("/user/saranvisa/scala-sbt-input/empid_name.txt").cache()
	
	# This action work as normal for the first time
	scala> dataRDD.count()
	res1: Long = 7
	
	# This action will be performed against in memory dataset
	scala> dataRDD.count()
	res2: Long = 7


4. Understanding Spark Caching
http://sujee.net/2015/01/22/understanding-spark-caching/#.WOlaZtzyuM8
http://spark.apache.org/screencasts/3-transformations-and-caching.html




# 
import org.apache.spark.storage.StorageLevel._

# Ex for MEMORY_ONLY
scala> val dataRDD = sc.textFile("/user/saranvisa/scala-sbt-input/empid_name.txt")
dataRDD1: org.apache.spark.rdd.RDD[String] = /user/saranvisa/scala-sbt-input/empid_name.txt MapPartitionsRDD[3] at textFile at <console>:30

scala> dataRDD.persist(MEMORY_ONLY)
res2: dataRDD1.type = /user/saranvisa/scala-sbt-input/empid_name.txt MapPartitionsRDD[3] at textFile at <console>:30

scala> dataRDD.collect()
res5: Array[String] = Array(1000,kumar, 1001,Raj, 1003,Adam, 1004,Alex, 1005,Jose, 1006,kumar, 1007,Alex)

========
# Ex for MEMORY_ONLY_SER

scala> val dataRDD1 = sc.textFile("/user/saranvisa/scala-sbt-input/empid_name.txt")
dataRDD1: org.apache.spark.rdd.RDD[String] = /user/saranvisa/scala-sbt-input/empid_name.txt MapPartitionsRDD[3] at textFile at <console>:30

scala> dataRDD1.persist(MEMORY_ONLY_SER)
res2: dataRDD1.type = /user/saranvisa/scala-sbt-input/empid_name.txt MapPartitionsRDD[3] at textFile at <console>:30

scala> dataRDD1.collect()
res3: Array[String] = Array(1000,kumar, 1001,Raj, 1003,Adam, 1004,Alex, 1005,Jose, 1006,kumar, 1007,Alex)





#######################################  UseCase16: DATA FRAME - Introduction #####################################################

http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#getting-started
https://hortonworks.com/hadoop-tutorial/a-lap-around-apache-spark/
https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL

## Topic: DataFrame: 
	a. DataFrame is a combination of RDD & Schema
	b. It is nothing but Collection of data organized into "named Columns" then use sql type of query on top of it

1. DATAFRAME uses sqlContext
	import org.apache.spark.sql.SQLContext
	
	# To be applied after declare: val sqlContext = new SQLContext(sc)
	// this is used to implicitly convert an RDD to a DataFrame.
	import sqlContext.implicits._
	
2. 	To create a dataframe from a JSON file or a Hive Table then use sqlContext as follows: 
	Ex:
			val df = sqlContext.jsonFile("path/people.json")	## This method is getting deprecated warning
			val df = sqlContext.read.json("path/people.json")	## NO Warning
			val df = sqlContext.sql("select * from qa.emp")
			
3. TextFile: To create a dataframe from TextFile, assign it to a RDD and create data from RDD using toDF(). Create a case class. 

	scala> val dataRDD = sc.textFile("/user/sksivanandan/empid_name.txt")
	dataRDD: org.apache.spark.rdd.RDD[String] = /user/sksivanandan/empid_name.txt MapPartitionsRDD[35] at textFile at <console>:28

	scala> dataRDD.collect().foreach(println)
	10,Kumar
	20,Sumitha
	30,Sachein
	40,Dhiya

	scala> val df = dataRDD.toDF()
	df: org.apache.spark.sql.DataFrame = [_1: string]

	scala> df.show
		+----------+
		|        _1|
		+----------+
		|  10,Kumar|
		|20,Sumitha|
		|30,Sachein|
		|  40,Dhiya|
		+----------+

4. In the above example, the entire row will be considered as a single column. So create a case class and use it as follows:

	scala> val dataRDD = sc.textFile("/user/sksivanandan/empid_name.txt")
	dataRDD: org.apache.spark.rdd.RDD[String] = /user/sksivanandan/empid_name.txt MapPartitionsRDD[35] at textFile at <console>:28

	scala> case class Empid_name(
			emp_id: Int,
			emp_name: String
			) 
  
	scala> val dataDF = dataRDD.map(rec => { val a = rec.split(",")
						Empid_name(a(0).toInt, a(1).toString()) }).toDF()
	
	scala> val dataDF = dataRDD.map(rec => { val a = rec.split(",")
     | Empid_name(a(0).toInt, a(1).toString()) }).toDF()
	dataDF: org.apache.spark.sql.DataFrame = [emp_id: int, emp_name: string]

	scala> dataDF.show
	+------+--------+
	|emp_id|emp_name|
	+------+--------+
	|    10|   Kumar|
	|    20| Sumitha|
	|    30| Sachein|
	|    40|   Dhiya|
	+------+--------+

	<<or>>
	
    scala> val dataDF = sc.textFile("/user/sksivanandan/empid_name.txt").
     | map(rec => {
     | val a = rec.split(",")
     | Empid_name(a(0).toInt, a(1).toString())
     | }).toDF()
	dataDF: org.apache.spark.sql.DataFrame = [emp_id: int, emp_name: string]

5. registerTempTable 

scala> dataDF.registerTempTable("tmp_empid_name")

scala> val dataRDD1 = sqlContext.sql("select * from tmp_empid_name")
dataRDD1: org.apache.spark.sql.DataFrame = [emp_id: int, emp_name: string]

scala> dataRDD1.collect().foreach(println)
[10,Kumar]
[20,Sumitha]
[30,Sachein]
[40,Dhiya]

6. To Convert DataFrame to RDD
	orderJoinDF.groupBy("order_date").agg(sum("order_item_subtotal")).
	sort("order_date").rdd.saveAsTextFile("/user/saranvisa/scala-sbt-output/orderdf")

7. To Convert RDD to DataFrame (Pre-request: case class)
		scala> val dataDF = dataRDD.map(rec => { val a = rec.split(",")
			   Empid_name(a(0).toInt, a(1).toString()) }).toDF()
	  
6. Dataframe Operations:
		a. printSchema
		b. show
		c. select
		d. groupBy
		e. agg
		f. sort
		g. filter
		h. registerTempTable
		and many more

7. Save Dataframe to different formats
	scala> val sqlc1 = sqlContext.jsonFile("/user/saranvisa/scala-sbt-input/empjson.json")
	scala> sqlc1.show
	+-----+-------+
	|empid|empname|
	+-----+-------+
	|   10|  kumar|
	|   20|  James|
	|   30|    Sam|
	|   40| Dinesh|	etc
	
	## Ignore the error mentioned below. Both write.parquet & write.save are storing file in parquet format
	scala> sqlc1.select("empid","empname").write.parquet("/user/saranvisa/scala-sbt-input/emp_parquet.parquet")
	scala> sqlc1.select("empid","empname").write.save("/user/saranvisa/scala-sbt-input/emp_parquet.parquet")
		SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
		SLF4J: Defaulting to no-operation (NOP) logger implementation
		SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
	
	## Save the file into ORC format
	scala> val results = sqlContext.sql("SELECT * FROM yahoo_stocks_temp")
	
	scala> results.write.format("orc").save("yahoo_stocks_orc")
	
	HDFS> hdfs dfs -ls /user/saranvisa/scala-sbt-input/emp_parquet.parquet	## Folder created with parquent data

## Practical-1
	val data = Array(1,2,3,4,5)
	val dataRDD = sc.parallelize(data)
	val dataDF = dataRDD.toDF()
	dataDF.write.save("/user/saranvisa/scala-sbt-input/temp/file1_parquet1")		
	dataDF.write.parquet("/user/saranvisa/scala-sbt-input/temp/file1_parquet2")
	val dataDF1 = sqlContext.read.parquet("/user/saranvisa/scala-sbt-input/temp/file1_parquet1")
	val dataDF2 = sqlContext.read.parquet("/user/saranvisa/scala-sbt-input/temp/file1_parquet2")
	dataDF1.show
	dataDF2.show
	
	
	
	
## Practical-2
		b. DATAFRAME uses sqlContext. 
			Ex: 
			import org.apache.spark.sql.functions._ 
			val df = sqlContext.jsonFile("people.json")
			val df = sqlContext.sql("select * from qa.emp")
			df: org.apache.spark.sql.DataFrame = [empid: int, empname: string]

			## val df = sqlContext.textFile("people.txt")    ## textFile will NOT work with sqlContext
			## value textFile is not a member of org.apache.spark.sql.SQLContext
			df.show
			df.select(df("name"), df("age") + 1).show()    # To increase the column value

## Practical-3			
	[saranvisa@gw01 ~]$ hdfs dfs -cat /user/saranvisa/scala-sbt-input/emp.json/part*
	{"empid":10,"empname":"kumar"}
	{"empid":20,"empname":"James"}

	## Data Frames will be performed on sqlContext
	import org.apache.spark.sql.SQLContext
	import org.apache.spark.sql.functions._ 

	val sqlContext = new SQLContext(sc)
	val df = sqlContext.jsonFile("/user/saranvisa/scala-sbt-input/emp.json")  ## Text file option not available, need to check

	## To print the Schema
	scala> df.printSchema()
	root
	|-- empid: long (nullable = true)
	|-- empname: string (nullable = true)

	## To select a particular column
	scala> df.select("empid").show()
+-----+
|empid|
+-----+
|   10|
|   20|
+-----+

	## To select a particular column
	scala> df.select("empid","empname").show()
+-----+-------+
|empid|empname|
+-----+-------+
|   10|  kumar|
|   20|  James|
+-----+-------+

	## Increment the value of EmpID by 1
	scala> df.select(df("empid")+1, df("empname")).show()
+-----------+-------+
|(empid + 1)|empname|
+-----------+-------+
|         11|  kumar|
|         21|  James|
+-----------+-------+

	## To check true or false
	scala> df.select(df("empid") > 10).show()
+------------+
|(empid > 10)|
+------------+
|       false|
|        true|
+------------+

	## To apply filter on DATA FRAME
	scala> df.filter(df("empid") > 10).show()
+-----+-------+
|empid|empname|
+-----+-------+
|   20|  James|
+-----+-------+

	## Apply group by
	scala> df.groupBy("empid").count().show()
+-----+-----+
|empid|count|
+-----+-----+
|   10|    1|
|   20|    1|
+-----+-----+

## Practical-4

	scala> import org.apache.spark.sql.SQLContext
	import org.apache.spark.sql.SQLContext

	scala> val df = sqlContext.sql("select * from qa.test13")
	df: org.apache.spark.sql.DataFrame = [var1: int, var2: decimal(5,2), var3: decimal(5,2)]

	scala> df.show
	+----+------+------+
	|var1|  var2|  var3|
	+----+------+------+
	| 150|  5.56|  0.36|
	|  10| 11.10| 22.30|
	|  13|211.10|422.13|
	+----+------+------+


#######################################  UseCase17: DATA FRAME - Compute total revenue for each day ###################################

## Use Case: compute total revenue for each day from Orders & Order_items tables using Data Frame

## Individual steps
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext


val conf = new SparkConf().setAppName("Total Revenue - Daily - Data Frames").setMaster("yarn-client")  
val sc = new SparkContext(conf)
val sqlContext = new SQLContext(sc)

case class Orders(
  order_id: Int,
  order_date: String,
  order_customer_id: Int,
  order_status: String) 

case class OrderItems(
  order_item_id: Int,
  order_item_order_id: Int,
  order_item_product_id: Int,
  order_item_quantity: Int,
  order_item_subtotal: Float,
  order_item_price: Float)  


val ordersDF = sc.textFile("/user/saranvisa/scala-sbt-input/orders/").map(rec => {val a = rec.split(",")
Orders(a(0).toInt, a(1).toString(), a(2).toInt, a(3).toString())
}).toDF()

## NOTE: use equal three times (===)
val ordersFilteredDF = ordersDF.filter(ordersDF("order_status")==="COMPLETE")

val orderItemsDF = sc.textFile("/user/saranvisa/scala-sbt-input/order_items").map(rec => {val a = rec.split(",")
OrderItems(a(0).toInt,
a(1).toInt,
a(2).toInt,
a(3).toInt,
a(4).toFloat,
a(5).toFloat)
}).toDF()

val orderJoinDF = ordersFilteredDF.join(orderItemsDF, ordersFilteredDF("order_id") === orderItemsDF("order_item_order_id") )

scala> orderJoinDF.printSchema
root
 |-- order_id: integer (nullable = false)
 |-- order_date: string (nullable = true)
 |-- order_customer_id: integer (nullable = false)
 |-- order_status: string (nullable = true)
 |-- order_item_id: integer (nullable = false)
 |-- order_item_order_id: integer (nullable = false)
 |-- order_item_product_id: integer (nullable = false)
 |-- order_item_quantity: integer (nullable = false)
 |-- order_item_subtotal: float (nullable = false)
 |-- order_item_price: float (nullable = false)

orderJoinDF.groupBy("order_date").agg(sum("order_item_subtotal")).sort("order_date").rdd.saveAsTextFile("/user/saranvisa/scala-sbt-output/orderdf")



#######################################  UseCase18: DATA FRAME - Join EMP and DEPT ###################################

##Tips: 
1. If you are creating more than one Case Class, make sure column name is NOT same in both the case classes. Otherwise it may give trouble when you join

scala> import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.SQLContext

scala> import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.hive.HiveContext



scala> case class EmpDept(
     | empid:Int,
     | empname:String,
     | deptid:Int)
defined class EmpDept

scala> val empDF = sc.textFile("/user/saranvisa/scala-sbt-input/empid_name_dept.txt").map(rec => {val a = rec.split(",")
     | EmpDept(a(0).toInt, a(1).toString(),a(2).toInt)
     | }).toDF()
empDF: org.apache.spark.sql.DataFrame = [empid: int, empname: string, deptid: int]

scala> empDF.show
+-----+-------+------+
|empid|empname|deptid|
+-----+-------+------+
| 1000|  kumar|    10|
| 1001|    Raj|    20|
| 1003|   Adam|    30|
| 1004|   Alex|    10|
| 1005|   Jose|    10|
| 1006|  kumar|    10|
| 1007|   Alex|    40|
+-----+-------+------+

scala> case class Dept(
     | deptid:Int,
     | deptname:String)
defined class Dept

scala> val deptDF = sc.textFile("/user/saranvisa/scala-sbt-input/dept.txt").map(rec => {val a = rec.split(",")
     | Dept(a(0).toInt, a(1).toString())
     | }).toDF()
deptDF: org.apache.spark.sql.DataFrame = [deptid: int, deptname: string]

scala> deptDF.show
+------+-----------+
|deptid|   deptname|
+------+-----------+
|    10| ITEngineer|
|    20|    Support|
|    30|    Manager|
|    40|ProdSupport|
+------+-----------+

scala> val empdeptJoinDF = empDF.join(deptDF,empDF("deptid")===deptDF("deptid"))
empdeptJoinDF: org.apache.spark.sql.DataFrame = [empid: int, empname: string, deptid: int, deptid: int, deptname: string]

scala> empdeptJoinDF.show
+-----+-------+------+------+-----------+
|empid|empname|deptid|deptid|   deptname|
+-----+-------+------+------+-----------+
| 1007|   Alex|    40|    40|ProdSupport|
| 1000|  kumar|    10|    10| ITEngineer|
| 1004|   Alex|    10|    10| ITEngineer|
| 1005|   Jose|    10|    10| ITEngineer|
| 1006|  kumar|    10|    10| ITEngineer|
| 1001|    Raj|    20|    20|    Support|
| 1003|   Adam|    30|    30|    Manager|
+-----+-------+------+------+-----------+

scala> val sqlContext = new HiveContext(sc)
sqlContext: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@5204a284

scala> empdeptJoinDF.registerTempTable("tmp_empdept")


<<<This is not working need to identify the issue>>> -- The initial understanding a package import is missing

scala> import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions._

##########################  UseCase18.1: How to define datatype when creating dataframe using sql.types ##########################

https://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/How-to-define-datatype-when-creating-dataframe-using-sql-types/m-p/53709

## Trying to convert a text file to DataFrame. I found using following method instead of case class. But where is the data type for each field is defined if we go by this method.

val people = sc.textFile("file:/home/edureka/dmishra/people.txt")
val schemaString = "name age"
import org.apache.spark.sql.Row;
import org.apache.spark.sql.types.{StructType,StructField,StringType};
val schema =
          StructType(
          schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, true)))
val rowRDD = people.map(_.split(",")).map(p => Row(p(0), p(1).trim))

val peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema)
peopleDataFrame.registerTempTable("people")
val results = sqlContext.sql("select name,age from people")
val r = results.map(t => "Name: " + t(0) + "Age : " + t(1)).collect().foreach(println

scala> results.dtypes.foreach(println)
(name,StringType)
(age,StringType)
 
# Where is the data type assigned for data frame. How to define age as integer data type in this case or if there is a date field, where to define it.

## Soln:

It is the below line which is setting the data types for both the fields as StringType: 
 
val schema =
          StructType(
          schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, true)))
You can define your custom schema as follows : 
 
val customSchema = StructType(Array(
    StructField("name", StringType, true),
    StructField("age", IntegerType, true)))
You can add additional fields as well in the above schema definition.
 
And then you can use this customSchema while creating the dataframe as follows: 
 
val peopleDataFrame = sqlContext.createDataFrame(rowRDD, customSchema)
 
Also for details, please see <get the link from community group link> page. 

#######################################  UseCase 19: Using Hive with ORC #######################################################

## Topic: ORC
	https://hortonworks.com/hadoop-tutorial/using-hive-with-orc-from-apache-spark/

## In Hive, Create ORC table from a normal table
	# Without row format
	> create table retail_stage.orders_orc stored as orc
		as select * from retail_stage.orders;
	
	# With row format
	> create table retail_stage.orders_orc 
		row format delimited fields terminated by ':'
		stored as orc
		as select * from retail_stage.orders;
	
## using Spark shell, reading data from HDFS, creating an Hive table in ORC format, querying the Hive Table, and persisting data using Spark SQL.

import org.apache.spark.sql.hive.orc._
import org.apache.spark.sql._

## Create hive Context
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

## Create ORC table
hiveContext.sql("create table yahoo_orc_table (date STRING, open_price FLOAT, high_price FLOAT, low_price FLOAT, close_price FLOAT, volume INT, adj_price FLOAT) stored as orc")

## Assign the data to RDD
val yahoo_stocks = sc.textFile("hdfs://sandbox.hortonworks.com:8020/tmp/yahoo_stocks.csv")
## Preview the data
yahoo_stocks.take(10)
## Separating the header from the data
val header = yahoo_stocks.first
## Now we need to separate the data into a new RDD where we do not have the header above and :
val data = yahoo_stocks.mapPartitionsWithIndex { (idx, iter) => if (idx == 0) iter.drop(1) else iter }

## CREATING A SCHEMA
case class YahooStockPrice(date: String, open: Float, high: Float, low: Float, close: Float, volume: Integer, adjClose: Float)

## ATTACHING THE SCHEMA TO THE PARSED DATA
val stockprice = data.map(_.split(",")).map(row => YahooStockPrice(row(0), row(1).trim.toFloat, row(2).trim.toFloat, row(3).trim.toFloat, row(4).trim.toFloat, row(5).trim.toInt, row(6).trim.toFloat)).toDF()

## REGISTERING A TEMPORARY TABLE
stockprice.registerTempTable("yahoo_stocks_temp")

## QUERYING AGAINST THE TABLE & The resultset returned from the Spark SQL query is now loaded in the results RDD
val results = sqlContext.sql("SELECT * FROM yahoo_stocks_temp")

## Print the RDD on the command line
results.map(t => "Stock Entry: " + t.toString).collect().foreach(println)

## SAVING AS AN ORC FILE...persist back the RDD into the Hive ORC table we created before.
results.write.format("orc").save("yahoo_stocks_orc")

## To store results in a hive directory rather than user directory, use this path instead:
/apps/hive/warehouse/yahoo_stocks_orc

## READING THE ORC FILE
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
val yahoo_stocks_orc = hiveContext.read.format("orc").load("yahoo_stocks_orc")


#######################################  UseCase 20: Configure Spark properties ###################################################

https://spark.apache.org/docs/1.6.1/configuration.html
http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/

#About Spark Configuration Properties
1. Default path /etc/spark/conf and file name spark-defaults.conf
2. These are default system properties included when running spark-submit.


## ITVersity (Ambari)
[saranvisa@gw01 conf]$ pwd
/etc/spark/conf
[saranvisa@gw01 conf]$ cat spark-defaults.conf
# Generated by Apache Ambari. Mon Apr 17 01:44:18 2017

spark.driver.extraClassPath /usr/share/java/mysql-connector-java.jar
spark.driver.extraLibraryPath /usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64
spark.eventLog.dir hdfs:///spark-history
spark.eventLog.enabled true
spark.executor.extraClassPath /usr/share/java/mysql-connector-java.jar
spark.executor.extraLibraryPath /usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64
spark.history.fs.logDirectory hdfs:///spark-history
spark.history.kerberos.keytab none
spark.history.kerberos.principal none
spark.history.provider org.apache.spark.deploy.history.FsHistoryProvider
spark.history.ui.port 18080
spark.yarn.containerLauncherMaxThreads 25
spark.yarn.driver.memoryOverhead 384
spark.yarn.executor.memoryOverhead 384
spark.yarn.historyServer.address gw01.itversity.com:18080
spark.yarn.max.executor.failures 3
spark.yarn.preserve.staging.files false
spark.yarn.queue default
spark.yarn.scheduler.heartbeat.interval-ms 5000
spark.yarn.submit.file.replication 3

## K12 (Cloudera Manager)
[sksivanandan@bda1node03 conf]$ pwd
/etc/spark/conf
[sksivanandan@bda1node03 conf]$ cat spark-defaults.conf
spark.authenticate=false
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.executorIdleTimeout=60
spark.dynamicAllocation.minExecutors=0
spark.dynamicAllocation.schedulerBacklogTimeout=1
spark.eventLog.enabled=true
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.shuffle.service.enabled=true
spark.shuffle.service.port=7337
spark.eventLog.dir=hdfs://k12bdacluster-ns/user/spark/applicationHistory
spark.yarn.historyServer.address=http://bda1node03.k12.com:18088
spark.yarn.jar=local:/opt/cloudera/parcels/CDH-5.7.0-1.cdh5.7.0.p1464.1349/lib/spark/lib/spark-assembly.jar
spark.driver.extraLibraryPath=/opt/cloudera/parcels/CDH-5.7.0-1.cdh5.7.0.p1464.1349/lib/hadoop/lib/native
spark.executor.extraLibraryPath=/opt/cloudera/parcels/CDH-5.7.0-1.cdh5.7.0.p1464.1349/lib/hadoop/lib/native
spark.yarn.am.extraLibraryPath=/opt/cloudera/parcels/CDH-5.7.0-1.cdh5.7.0.p1464.1349/lib/hadoop/lib/native
spark.yarn.config.gatewayPath=/opt/cloudera/parcels
spark.yarn.config.replacementPath={{HADOOP_COMMON_HOME}}/../../..
spark.master=yarn-client

#######################################  UseCase 21: Spark properties with Security##########################################

https://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/

spark-submit --master yarn --deploy-mode cluster \
     --conf spark.yarn.maxAppAttempts=4 \
     --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
     --conf spark.yarn.max.executor.failures={8 * num_executors} \
     --conf spark.yarn.executor.failuresValidityInterval=1h \
     --conf spark.task.maxFailures=8 \
     --queue realtime_queue \
     --conf spark.speculation=true \
     --principal user/hostname@domain \
     --keytab /path/to/foo.keytab
	 
spark-submit --master yarn --deploy-mode cluster \
     --conf spark.yarn.maxAppAttempts=4 \
     --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
     --conf spark.yarn.max.executor.failures={8 * num_executors} \
     --conf spark.yarn.executor.failuresValidityInterval=1h \
     --conf spark.task.maxFailures=8 \
     --queue realtime_queue \
     --conf spark.speculation=true \
     --principal user/hostname@domain \
     --keytab /path/to/foo.keytab \
     --conf spark.hadoop.fs.hdfs.impl.disable.cache=true	 

#######################################  UseCase 22: DATA SETS ##############################################################
http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#getting-started
https://www.youtube.com/watch?v=ykzjRMgYE3M		

## 1. Datasets are similar to RDDs, however, instead of using Java Serialization they use a specialized "Encoder" to serialize the objects for processing		
## 2. encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing 



		
########################################################   ITVersity - AvgRevenueDaily   ##################################		
package retail

import org.apache.spark.SparkContext, org.apache.spark.SparkConf
import com.typesafe.config._
import org.apache.hadoop.fs._

object AvgRevenueDaily {
  def main(args: Array[String]) {
    val appConf = ConfigFactory.load()
    val conf = new SparkConf().
      setAppName("Average Revenue - Daily").
      setMaster(appConf.getConfig(args(2)).getString("deploymentMaster"))
    val sc = new SparkContext(conf)
    val inputPath = args(0)
    val outputPath = args(1)

    val fs = FileSystem.get(sc.hadoopConfiguration)
    val inputPathExists = fs.exists(new Path(inputPath))
    val outputPathExists = fs.exists(new Path(outputPath))

    if (!inputPathExists) {
      println("Input Path does not exists")
      return
    }

    if (outputPathExists) {
      fs.delete(new Path(outputPath), true)
    }

    val ordersRDD = sc.textFile(inputPath + "/orders")
    val orderItemsRDD = sc.textFile(inputPath + "/order_items")

    val ordersCompleted = ordersRDD.
      filter(rec => (rec.split(",")(3) == "COMPLETE"))

    val orders = ordersCompleted.
      map(rec => (rec.split(",")(0).toInt, rec.split(",")(1)))
    val orderItemsMap = orderItemsRDD.
      map(rec => (rec.split(",")(1).toInt, rec.split(",")(4).toFloat))

    val orderItems = orderItemsMap.
      reduceByKey((acc, value) => acc + value)

    val ordersJoin = orders.join(orderItems)

    val ordersJoinMap = ordersJoin.map(rec => (rec._2._1, rec._2._2))

    val revenuePerDay = ordersJoinMap.aggregateByKey((0.0, 0))(
      (acc, value) => (acc._1 + value, acc._2 + 1),
      (total1, total2) => (total1._1 + total2._1, total1._2 + total2._2))

    val averageRevenuePerDay = revenuePerDay.
      map(rec => (rec._1, BigDecimal(rec._2._1 / rec._2._2).
        setScale(2, BigDecimal.RoundingMode.HALF_UP).toFloat))

    val averageRevenuePerDaySorted = averageRevenuePerDay.
      sortByKey()

    averageRevenuePerDaySorted.
      map(rec => rec._1 + "," + rec._2).
      saveAsTextFile(outputPath)

  }
}

########################################################   ITVersity - word count   ##################################		

package wordcount

import com.typesafe.config._
import org.apache.spark.SparkContext, org.apache.spark.SparkConf
import org.apache.hadoop.fs._


object WordCount {
  def main(args: Array[String]) {
    val appConf = ConfigFactory.load()
    val conf = new SparkConf().
      setAppName("Word Count").
      setMaster(appConf.getString("deploymentMaster"))
      
    for(c < - conf.getAll)
      println(c._2)
    val sc = new SparkContext(conf)
    val inputPath = args(0)
    val outputPath = args(1)
    
    val fs = FileSystem.get(sc.hadoopConfiguration)
    val inputPathExists = fs.exists(new Path(inputPath))
    val outputPathExists = fs.exists(new Path(outputPath))
    
    if(!inputPathExists) {
      println("Invalid input path")
      return
    }
      
    if(outputPathExists)
      fs.delete(new Path(outputPath), true)
      
    val wc = sc.textFile(inputPath).
      flatMap(rec => rec.split(" ")).
      map(rec => (rec, 1)).
      reduceByKey((acc, value) => acc + value)
      
    wc.saveAsTextFile(outputPath)

  }
}

############################################   Pending: Spark 2.0.1 option ###########################################

https://spark.apache.org/docs/2.0.1/api/java/org/apache/spark/sql/SparkSession.html
The builder can also be used to create a new session:
   SparkSession.builder()
     .master("local")
     .appName("Word Count")
     .config("spark.some.config.option", "some-value").
     .getOrCreate()

https://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/spark-dataframe-write-to-file-using-scala/m-p/54124	 
I am trying to read a file and add two extra columns. 1. Seq no and 2. filename. When I run spark job in scala IDE output is generated correctly but when I run in putty with local or cluster mode job is stucks at stage-2 (save at File_Process). There is no progress even i wait for an hour. I am testing on 1GB data.
Below is the code i am using

object File_Process
{
 Logger.getLogger("org").setLevel(Level.ERROR)  
 val spark = SparkSession.builder().master("yarn").appName("File_Process").getOrCreate()
 
 def main(arg:Array[String])
 {
 
  val FileDF = spark.read.csv("/data/sourcefile/")
  val rdd = FileDF.rdd.zipWithIndex().map(indexedRow => Row.fromSeq((indexedRow._2.toLong+SEED+1)+:indexedRow._1.toSeq))
  val FileDFWithSeqNo = StructType(Array(StructField("UniqueRowIdentifier",LongType)).++(FileDF.schema.fields))
  val datasetnew = spark.createDataFrame(rdd,FileDFWithSeqNo)
  val dataframefinal = datasetnew.withColumn("Filetag", lit(filename))
  val query = dataframefinal.write
              .mode("overwrite")
              .format("com.databricks.spark.csv")
              .option("delimiter", "|")
              .save("/data/text_file/")
  spark.stop()
 }
 

###########################   Pending: Apache Spark - Scala - Externalizing Parameters (To pass arguments from a file)  ##################

Apache Spark - Scala - Externalizing Parameters (To pass arguments from a file)
https://www.youtube.com/watch?v=ln4dpChqJPI&list=PLf0swTFhTI8pgc4wyhT8lIEJCPT08RI3X&index=35

	# To pass the arguments from a file, NOTE the version for typesafe
	libraryDependencies += "com.typesafe" % "config" % "1.3.0"


################################## Chapter?: Spark program to compare source to target  #############################

https://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/How-to-optimize-Spark-application-in-case-table-with-too-many/m-p/52523

I have a simple Spark application for data validation between source like DB2,Oracle and target Hive. My application is working fine when running against table with columns around 200-250 but same code starts lagging and take too much time to complete when run for table with 400 columns and 9 million records or more. I have attached complete code (tried in yarn-client and yarn-cluster mode both.) Below is the function which is comparing data of two tables	
	
def compareTable(sourceTable:String,targetTable:String,applicationName:String,toolPath:String,result:PrintStream)={
try{
val dbProperties = new Properties()
val propFile = "file:"+"//"+toolPath+"/"+applicationName+".properties"
dbProperties.load(new URL(propFile).openStream())
val connectionURL = dbProperties.getProperty("connURL")
val password = dbProperties.getProperty("password")
val userName = dbProperties.getProperty("userName")
val driver = dbProperties.getProperty("driver")
val sourceSchema = dbProperties.getProperty("sourceSchema")
val targetSchema = dbProperties.getProperty("targetSchema")
dbProperties.setProperty("user",userName);
dbProperties.setProperty("password",password);
dbProperties.setProperty("driver",driver);
val sourceDF = hiveContext.read.jdbc(connectionURL,sourceTable,dbProperties)
val hiveQuery = "select * from "+ targetSchema+"."+targetTable
val targetDF = hiveContext.sql(hiveQuery)
val sourceColumnSet:Array[(String,String)] = sourceDF.dtypes
val targetColumnSet:Array[(String,String)] = targetDF.dtypes
val columnSets =  compareSchema(targetColumnSet,sourceColumnSet,targetTable) //"This compare schema of tables and return column sets."
sourceDF.registerTempTable("sourceTable")
targetDF.registerTempTable("targetTable")
val sourceQuery = "select "+columnSets._1+" from sourceTable"
val targetQuery = "select "+columnSets._2+" from targetTable"
val SDF = hiveContext.sql(sourceQuery)
val TDF = hiveContext.sql(targetQuery)
val misMatchedRecords = TDF.except(SDF)
val misMatchedRecordsCount = misMatchedRecords.count
if ( misMatchedRecordsCount > 0){
println("Mismatched Records of target table: "+targetTable)
misMatchedRecords.show()
}
else{
println("All records are correct in target table: " + targetTable)
}	



#################################################### K12 - BKT Calculator ####################################################
## BKTJob.scala

package com.k12.bkt.calculator

import java.io.File

import com.typesafe.config.ConfigFactory
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

object BKTJob {
  /*
  https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL
  http://stackoverflow.com/questions/31980584/how-to-connect-to-a-hive-metastore-programmatically-in-sparksql
   */
   
   /*
   # The below is something similar to 
   val or val VariableName : DataType = [Initial Value]
   val myVal :String;   
   */
  var input_table: org.apache.spark.broadcast.Broadcast[String] = _
  var output_table: org.apache.spark.broadcast.Broadcast[String] = _
  var input_tables_schema: org.apache.spark.broadcast.Broadcast[String] = _
  var output_tables_schema: org.apache.spark.broadcast.Broadcast[String] = _

  def main(args: Array[String]) {
    var config: com.typesafe.config.Config = ConfigFactory.empty()
    if (args.length < 1)
      println("Missing properties file")
    else
    //Example of access: config.getString("username")
      config = get_config(args(0))
    /* Dynamically Loading Spark Properties
     ./bin/spark-submit --name "Spark-Hive-bkt-calculator" --master local[4] --conf spark.eventLog.enabled=false
  --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" bkt-calculator-0.0.1-SNAPSHOT.jar
     */
    val sc: org.apache.spark.SparkContext = new SparkContext(new SparkConf())
    val sql_context: SQLContext = new HiveContext(sc)
    input_table = sc.broadcast(config.getString("input_table"))
    output_table = sc.broadcast(config.getString("output_table"))
    input_tables_schema = sc.broadcast(config.getString("input_tables_schema"))
    output_tables_schema = sc.broadcast(config.getString("output_tables_schema"))
    // this is used to implicitly convert an RDD to a DataFrame.
    runJob(sql_context)
    sc.stop()
  }

  private def get_config(file_location: String): com.typesafe.config.Config = {
    try {
      ConfigFactory.parseFile(new File(file_location))
    } catch {
      case e: Exception => {
        e.printStackTrace()
        ConfigFactory.empty()
      }
    }
  }

  private def runJob(sql_context: org.apache.spark.sql.SQLContext): Unit = {
    val input_table_str = input_table.value
    val input_tables_schema_str = input_tables_schema.value
    val query_dataframe = sql_context.sql(s"SELECT * FROM $input_tables_schema_str.$input_table_str")
    val rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = query_dataframe.rdd
    // Create a groupId for each row so the (questionId + studentId)
    val mapped_group_ids = rows.map(rv => ("S-" + rv.getAs[Int]("identity_id").toString + "-OBJ-" + rv.getAs[Int]("master_objective_id").toString, rv))
    // For each groupId (questionId + studentId) create a new RDD with the form ( (questionId + studentId), [questionsScores, .... ] )
    // returns a dataset of (K, List[org.apache.spark.sql.Row] ) pairs
    val questions_groups = mapped_group_ids.groupByKey().mapValues(_.toList).mapValues(v_list => get_bkt(v_list)).flatMap { case (key, values) => values.map((key, _)) }.map(v => v._2)
    /* -- output TABLE structure -- */
    val table_structure: org.apache.spark.sql.types.StructType = org.apache.spark.sql.types.StructType(
      StructField("master_objective_id", LongType, nullable = false) ::
        StructField("identity_id", LongType, nullable = false) ::
        StructField("assessment_id", StringType, nullable = false) ::
        StructField("date_submitted", StringType, nullable = false) ::
        StructField("assmtitem_attempt_id", LongType, nullable = false) ::
        StructField("proficiency", DoubleType, nullable = false) ::
        StructField("l0", DoubleType, nullable = false) :: Nil
    )
    sql_context.createDataFrame(questions_groups, table_structure).registerTempTable("tmp_out_table_bkt")
    val output_table_str = output_table.value
    val output_tables_schema_str = output_tables_schema.value
    //http://stackoverflow.com/questions/17425492/hive-insert-query-like-sql
    sql_context.sql(s"INSERT INTO $output_tables_schema_str.$output_table_str SELECT * FROM tmp_out_table_bkt")
  }


  private def get_bkt(rows: List[org.apache.spark.sql.Row]): scala.collection.immutable.List[org.apache.spark.sql.Row] = {
    var questions_list = List[scala.collection.mutable.Map[String, Any]]()
    for (q <- rows) {
      /* for each question in the INPUT HIVE table */
      val q_el = scala.collection.mutable.Map[String, Any](
        "completion_time" -> q.getAs[Long]("completion_time"), //Parse from STRING to long
        "question_score_obtained" -> q.getAs[Int]("question_score_obtained"),
        "question_full_score" -> q.getAs[Int]("question_full_score"),
        "identity_id" -> q.getAs[Int]("identity_id"),
        "master_objective_id" -> q.getAs[Int]("master_objective_id"),
        "attempt_number" -> q.getAs[Int]("attempt_number"),
        "assessment_id" -> q.getAs[String]("assessment_id"),
        "assessment_type" -> q.getAs[String]("assessment_type"),
        "sequence_number" -> q.getAs[Int]("sequence_number")
      )
      questions_list = q_el +: questions_list
    }
    val bkt_calculator = new com.k12.bkt.calculator.tasks.SimpleBayesianKnowledgeTracing()
    val result_values: scala.collection.immutable.List[com.k12.bkt.calculator.data.ObjectiveBKT] = bkt_calculator.compute(questions_list)

    result_values.map { row =>
      val date_str = new org.joda.time.DateTime(row.timestamp).toString("yyyy-MM-dd HH:mm:ss")

      org.apache.spark.sql.Row(
        row.objectiveId,
        row.identityId,
        row.assessmentId,
        date_str,
        row.attemptNumber,
        row.proficiency,
        row.l0
      )
    }
  }
}
################################################## Connect HBase from Scala #############################################

Pre-request: Make sure  /etc/hbase/conf/hbase-site.xml

https://community.cloudera.com/t5/Data-Ingestion-Integration/Problem-in-connecting-Hbace-from-Scala-code-in-Cloudera-Quick/m-p/52773
Getting Issue - To be fixed
The reason for the issue could be : Either you have to copy/paste hbase-site.xml to /etc/spark/conf (or) create a softlink and try again

https://hbase.apache.org/book.html#scala

package main.scala

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{ConnectionFactory,HTable,Put}
import org.apache.hadoop.hbase.util.Bytes

object Hi {

 def main(args: Array[String]) = {
 println("Hi!")
 val conf:Configuration = HBaseConfiguration.create()
 conf.set("hbase.zookeeper.quorum", "xxx.xxx.xxx.xxx") //IP address of my Cloudera virtual machine
 conf.set("hbase.zookeeper.property.clientPort", "2181")
 val table:HTable = new HTable(conf, "emp1")
 val put1:Put = new Put(Bytes.toBytes("row1"))
 put1.add(Bytes.toBytes("personal_data"),Bytes.toBytes("qual1"),Bytes.toBytes("val1"))
 table.put(put1)
 println("Success")
 }
} 

## SBT
name := "play-sbt-project"
version := "1.0"
scalaVersion := "2.10.2"
resolvers += "Apache HBase" at "https://repository.apache.org/content/repositories/releases"
resolvers += "Thrift" at "http://people.apache.org/~rawson/repo/"
libraryDependencies ++= Seq(
"org.apache.hadoop" % "hadoop-core" % "1.2.1",
"org.apache.hbase" % "hbase" % "1.2.0",
"org.apache.hbase" % "hbase-client" % "1.2.0",
"org.apache.hbase" % "hbase-common" % "1.2.0",
"org.apache.hbase" % "hbase-server" % "1.2.0"
)
